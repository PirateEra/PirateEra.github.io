# Note to self, the first 3 projects will be highlighted

- id: pirate-era
  date: "2019 - Present"
  title: "Pirate-Era MMORPG"
  description: "Solo-developed MMORPG in Minecraft with 30,000+ players, featuring custom plugins, cross-server infrastructure, and data-driven game balancing."
  tools: "Java | Python | Denizen | MySQL | Redis | Server Administration"
  about: "Pirate-Era is a full-scale MMORPG built within Minecraft, attracting over 30,000 unique members. As the sole developer since 2019, I am responsible for all aspects of the project, including backend infrastructure, gameplay systems, and development tools. This involved creating custom Java plugins, writing Python scripts for automation and data analysis, and designing all in-game mechanics from scratch. I also self-host the entire project on a dedicated Linux device, managing all security, database (MySQL/Redis), and network setup."
  image: "/assets/images/pirate-era-cover.png"
  url: "/pages/projects/pirate-era/"
  contributions:
    Infrastructure & Backend:
      - "Built a cross-server player system using Redis to sync player data (levels, skills, inventory) across lobbies, enabling scaling across multiple server."
      - "Self-hosted the project on a dedicated Linux device, managing all server security, network configuration, and database (MySQL/Redis) administration."
    Data-Driven Balancing & Tooling:
      - "Developed a Java-Python bridge to analyze player data, enabling informed, data-driven decisions for balancing classes, the economy, bosses, and skills. [Read More](/pages/projects/pirate-era/pirate_era_balancing/)"
      - "Wrote a Python tool to automatically generate boss textures from a player skin, removing the need for manual UV mapping. [Read More](/pages/projects/pirate-era/pirate_era_bossgenerator/)"
    Modular Gameplay Systems:
      - "Designed a modular skill casting system for PVE/PVP, including an in-game tool for creating complex skills without coding. [Read More](/pages/projects/pirate-era/pirate_era_skill_system/)"
      - "Created a modular boss system that utilizes the player skill system, also managed by an in-game configuration tool. [Read More](/pages/projects/pirate-era/pirate_era_boss_system/)"
      - "Wrote a dynamic, template-based dialogue system for creating NPC interactions through a simple menu interface."
    Features & UI:
      - "Engineered an automatic bounty poster system using Python to generate bounty posters from player skin data, displayed in-game via Java/Denizen. [Read More](/pages/projects/pirate-era/poster_system/)"
      - "Implemented a custom HUD shader to display a unique, non-vanilla interface on the player's client. [Read More](/pages/projects/pirate-era/hud_shader/)"
      - "Created an in game wiki that auto-generates documentation for items, quests, and systems by parsing the server code."
  media:
    - "https://www.youtube.com/watch?v=osgt67R6ag0"
    - "https://www.youtube.com/watch?v=nzzOIViL7GU"

- id: cargoprobe
  date: "02-2023"
  title: "Shipping Container Delay Prediction"
  description: "Developed a regression model to predict shipping container ETA delays for CargoProbe, improving on their existing heuristics with a Random Forest."
  tools: "Python | Scikit-learn | Random Forest | Data Analysis | Data Processing"
  about: "This project for CargoProbe tackled supply chain disruptions by predicting shipping container delays. The provided data was complex, with over 330k entries, 23 columns, and significant missing values (e.g., 98% missing port data). After extensive data cleaning, feature engineering (creating 'time_left', 'time_spent', 'delay'), and imputation analysis, we compared several regression models. The final Random Forest model significantly outperformed CargoProbe's existing heuristics, reducing the Mean Absolute Error from 3.58 days to 1.20 days and the MSE from 54.75 to 7.92."
  image: "/assets/images/cargoprobe.png"
  url: "/pages/projects/cargoprobe/"
  project_pdf_title: "Technical Report"
  project_pdf: "/assets/pdfs/cargoprobe.pdf"
  contributions:
    Data Cleaning and Preprocessing:
      - "Analyzed and cleaned a large dataset of 331,956 entries, handling extensive missing values (NaNs) which constituted almost half the data."
      - "Standardized timestamp data to UTC and converted string values to lowercase to resolve duplicates and misspellings."
      - "Managed data structure by merging rows to fill missing values and removing duplicate updates to prevent model bias, resulting in 18,772 clean rows for training."
      - "Performed a chronological train-test split (80-20) instead of random shuffling to prevent data leakage from the same trip appearing in both sets."
    Feature Engineering and Data Analysis:
      - "Engineered critical time-based features for regression, including 'time_left' (ETA - update_datetime) and 'time_spent' (update_datetime - date_added)."
      - "Created the target variable 'delay' (ATA - ETA) to quantify the prediction error of CargoProbe's existing system."
      - "Analyzed the cleaned data, finding that 84.53% of trips arrived late and 28.26% were delayed by 72 hours or more."
      - "Applied One-Hot Encoding to categorical features like 'origin_city', 'vesselname', and 'activity' to prepare data for modeling."
    Model Development and Evaluation:
      - "Evaluated multiple regression models, including Linear Regression (which underfit) and Polynomial Regression (which was too computationally expensive)."
      - "Selected, trained, and tuned a Random Forest regression model as the final, most robust solution."
      - "Used RandomizedSearchCV to find optimal hyperparameters for the Random Forest, balancing the model and reducing the MSE for outliers."

- id: graphunderstanding
  date: "06-2025"
  title: "LLM Commentary for Graph Understanding"
  description: "Interactive graph visualization system to query graphs using natural language, providing LLM explanations and significance scores for query terms."
  tools: "Python | Dash | Plotly | Pandas | Ollama"
  about: "The system proposes an interactive, user-friendly visual interface, allowing users to retrieve graphs using natural language queries. It enhances interpretability by providing comprehensive explanations from the LLM and offering phrase-level feedback (significance scores) on how influential noun, verb, and preposition phrases were in generating the resulting subgraph."
  image: "/assets/images/graph_cover.png"
  url: "/pages/projects/graphunderstanding/"
  github-link: https://github.com/PirateEra/Multimedia-Analytics
  project_pdf_title: "Technical Report"
  project_pdf: "/assets/pdfs/graphunderstanding.pdf"
  contributions:
    Interactive Interface Development:
      - "Developed an interactive and user-friendly visual system for natural language graph querying using Dash and Plotly."
      - "Visualized the original graphs and their retrieved subgraphs in the interface's center part."
    Query Interpretability & Feedback:
      - "Implemented phrase-level feedback over the user's query by providing significance scores to show how relevant a phrase was for retrieving the subgraph."
      - "Integrated the significance score calculation using an entity-level perturbation strategy and the Jaccard similarity between the original and perturbed subgraphs."
    LLM Integration & Explanation:
      - "Utilized an LLM (Llama3.2:3b via Ollama) to answer user queries based on the retrieved subgraph."
      - "Provided an explanation alongside the generated answer, detailing how the LLM arrived at its conclusion."
    Data Analysis and Interpretation:
      - "Developed a novel method for phrase-level feedback by calculating significance scores for query terms to enhance model interpretability."
      - "Employed the Entity-level Perturbation strategy to modify queries and determine the influence of specific lexical units on subgraph retrieval."
      - "Quantified entity influence by calculating the similarity (using Jaccard index concept) between the original and perturbed subgraphs."
  presentation_title: "Visual Interface Screenshots"
  presentation_folder: "/assets/images/projects/graphvisualisation/"

- id: hightide
  date: "01-2017"
  title: "High Tide"
  description: "Co-Owner of a multiplayer survival game in Unity3D set in a medieval world."
  image: "/assets/images/hightide_cover.png"
  url: "/pages/projects/hightide"
  tools: "Unity3D | C# | Networking | MySQL | Multiplayer"
  about: |
    High Tide was a self-started online multiplayer survival game co-developed with a friend. Players built castles and competed on official servers. The world underwent server wipes triggered by an incoming tide, forcing naval combat during these events. The project spanned 18-24 months and was my first serious game project, where I gained deep experience in Unity3D, networking, and gameplay systems.
  media:
    - "https://www.youtube.com/watch?v=SvbI4OyGvQU"
    - "https://www.youtube.com/watch?v=vE84TJUep0Y"
    - "https://www.youtube.com/watch?v=6icDc453A_o"
    - "/assets/videos/hightidebuildsystem.mp4"
    - "/assets/videos/hightide_swimming.mp4"
    - "/assets/videos/hightide_fighting.mp4"
  contributions:
    Character Controller:
      - "Developed a 3D character controller capable of multiple stances: passive, fighting, weapon-specific stances, swimming."
      - "Implemented smooth stance switching and animation blending for gameplay fluidity."
    Building System:
      - "Created a modular building system using block shapes with smart snapping for correct materials."
      - "Enabled players to construct structures in the world, saving them persistently on the server."
    Networking & Multiplayer:
      - "Implemented client-server architecture to sync player actions, bases, and inventories using MySQL."
      - "Developed PVP mechanics allowing players to damage each other and interact dynamically in the world."

# - id: hightide
  # logo: "/assets/images/projects/mock/mockimg.png"
  # title: "High Tide"
  # description: "Co-Owner of a survival game made in Unity3D which sadly never launched"
  # image: "/assets/images/projects/mock/mockimg.png"
  # url: "/pages/projects/hightide"
  # tools: "Unity3D | C# | Two-man team"
  # about: "High Tide is a Unity3D surival game. It plays off in a medieval setting where players build castles and compete with each other on official servers"
  # project_pdf_title: "Report"
  # project_pdf: "/assets/pdfs/ATCS_Report.pdf"
  # presentation_title: "example title"
  # presentation_folder: "/assets/slides/ai4mi/"
  # media:
  #   - "https://www.youtube.com/watch?v=SvbI4OyGvQU"
  #   - "https://www.youtube.com/watch?v=vE84TJUep0Y"
  #   - "/assets/videos/hightidebuildsystem.mp4"
  #   - "/assets/videos/hightide_swimming.mp4"
  #   - "/assets/videos/hightide_fighting.mp4"
  # contributions:
  #   Character Controller:
  #     - "Implemented movement and camera system"
  #     - "Added sprinting and crouching mechanics"
  #   Animation Controller:
  #     - "Integrated state-based animation system"
  #     - "Synced animation transitions with gameplay logic"
  #   Building System:
  #     - "Created a modular building system"
  #     - "Allowed to create and build houses in a block based setting"
  # code_snippets_title: Code Samples
  # code_snippets:
  #   - title: "C# example"
  #     content: |
  #       ```csharp
  #       void Update() {
  #         if (Input.GetKey(KeyCode.Space)) Jump();
  #       }
  #       ```
  #   - title: "Python example"
  #     content: |
  #       ```python
  #       def example_code(speed):
  #           print(f"Moving at {speed} speed!")
  #       ```

- id: "ood-generalization"
  title: "ACTS"
  date: "05-2025"
  title: "OoD Prediction for Emotion Transfer"
  description: "A transfer learning project using Task and Text Embeddings to predict model performance across different emotion classification datasets (Out-of-Distribution, OoD)."
  tools: "Python | DeBERTaV3-base | Task Embeddings | Transfer Learning"
  about: "The core challenge addressed is the difficulty of generalizing emotion classifiers across datasets due to varied labeling schemes. This project predicted the effectiveness of intermediate dataset transfer by building a regression model. The prediction variables were based on the similarity between datasets, quantified using class-conditional Task and Text Embeddings derived from gradient information, to determine which datasets serve as effective pre-training corpora."
  image: "/assets/images/atcs.png"
  url: "/pages/projects/ood-generalisation/"
  github-link: https://github.com/PirateEra/ood-generalization-transfer
  project_pdf_title: "Technical Report"
  project_pdf: "/assets/pdfs/ood-generalization.pdf"
  contributions:
    Transfer Learning and Model Prediction:
      - "Designed a pipeline to predict transfer performance (OoD Generalization) between emotion datasets without requiring full model evaluation."
      - "Fine-tuned a DeBERTaV3-base transformer encoder on 10 source emotion datasets in isolation to learn dataset-specific text representations."
      - "Developed a regression model to predict transferability loss based on dataset similarity metrics."
    Advanced Representation Learning:
      - "Implemented the construction of Task Embeddings using the squared gradient norm of the frozen encoder to capture dataset-specific structure."
      - "Created class-conditional representations for each emotion in each dataset, using both Task and Text Embeddings to quantify dataset similarity."
    Data Analysis and Visualization:
      - "Applied dimensionality reduction techniques to visualize the constructed Task Embeddings and discover structure within different emotional classification corpora."
      - "Addressed challenges from diverse theoretical frameworks and varied, non-overlapping label sets across multiple emotional classification corpora."
  media:
    - "/assets/images/atcs_poster.jpg"

- id: affiliaterobot
  date: "07-2023"
  title: "Affiliate Robot"
  description: "AI-driven web application that automates the generation of affiliate marketing articles using web scraping, dynamic prompting, and the OpenAI API."
  tools: "Python | PHP | OpenAI API | Web Scraping | HTML/CSS/JS"
  about: "Developed the core logic for Affiliaterobot.nl, a Minimum Viable Product (MVP) aimed at automating content creation for affiliate marketers. The system generated 'Top X' style articles by dynamically gathering product data via web scraping and using prompt engineering to synthesize the information into a structured, engaging article format. The site integrated user-specific affiliate links for revenue generation."
  image: "/assets/images/affiliate_robot_cover.png"
  url: "/pages/projects/affiliate-robot/"
  project_pdf_title: "Technical Report"
  project_pdf: "/assets/pdfs/affiliaterobot.pdf"
  contributions:
    AI and Dynamic Prompt Engineering:
      - "Authored the primary logic for the Article Generator, orchestrating the sequence from data acquisition to final LLM output."
      - "Implemented dynamic prompt creation using web-scraped data to guide the OpenAI API in generating contextually rich and accurate 'Top 10' style articles."
      - "Focused on prompt engineering to ensure the LLM output was structured correctly (e.g., using proper HTML) and aligned with the article's tone and format."
    Full-Stack Backend Integration:
      - "Developed a custom WordPress plugin in PHP to manage the entire back-end logic, including handling user input, and triggering the article generation process."
      - "Integrated the Python web scraping module with the PHP back-end to seamlessly pass scraped product data for prompt construction."
    Data Acquisition and Web Scraping:
      - "Designed and implemented the Python web scraping module targeting Bol.com to efficiently gather real-time data (product names, descriptions, prices, etc.) for dynamic content."
    Front-End Logic and MVP Development:
      - "Implemented the core article generator functionality using a combination of HTML, CSS, JavaScript, and PHP to structure the final article presentation on the front-end, supporting the full MVP requirements (account/login, options)."
  presentation_title: "Visual Interface Screenshots"
  presentation_folder: "/assets/slides/affiliaterobot/"

- id: roger
  title: "Robust AI-Generated Image Detection"
  date: "05-2025"
  description: "Developed ROGER, a robust, multi-modal model for AI-generated image detection that outperforms a state of the art model (SPAI) on augmented real-world images."
  tools: "Python | PyTorch | Selenium | Deep Learning"
  image: "/assets/images/roger_pipeline.png"
  url: "/pages/projects/roger/"
  project_pdf_title: "Technical Report"
  project_pdf: "/assets/pdfs/roger.pdf"
  github-link: https://github.com/PirateEra/ROGER
  contributions:
    Data Pipeline and Robustness Testing:
      - "Contributed to creating challenging datasets that simulate real-world image modifications to test model robustness."
      - "Specifically implemented the social media screenshot simulation dataset, using Selenium to dynamically inject images into a custom HTML template."
      - "Randomized numerous dynamic HTML elements (like/comment counts, usernames, captions) to ensure each generated screenshot was unique."
    Model Development (ROGER):
      - "Co-developed the ROGER (RObust AI-GEnerated image Recognizer) model to address the weaknesses of SPAI."
      - "Designed and implemented the integrated model approach, a multi-modal architecture combining three complementary AID models."
      - "Extracted and concatenated embeddings from SPAI (spectral), RINE (mid-level), and PatchCraft (texture) into a unified 768-dimensional feature vector."
      - "Trained the final MLP classification head (2-layer, 1536-unit) on the combined embeddings to produce a single, robust prediction."
    Performance Evaluation:
      - "Successfully reproduced the original SPAI paper's results, verifying its baseline performance and high reproducibility."
      - "Demonstrated that the created ROGER model significantly outperforms the baseline SPAI on all four challenging real-world datasets."
  media:
    - "/assets/images/roger_pipeline.png"

- id: medai
  date: "10-2025"
  title: "Improving 3D CT Organ Segmentation"
  description: "An AI for Medical Imaging project that improved 3D segmentation of thoracic organs by systematically addressing severe class imbalance and model noise."
  tools: "Python | PyTorch | Data Analysis | Loss Functions | Jupyter"
  about: "This project's goal was to improve upon a baseline ENet model for 3D segmentation of organs (Heart, Aorta, Esophagus, Trachea) in CT scans. The primary challenges were severe class imbalance and noisy predictions. I analyzed and implemented various techniques, a key component of this was a deep dive into custom loss functions and data analysis to mitigate imbalance."
  image: "/assets/images/ai4mi.png"
  url: "/pages/projects/medai/"
  github-link: https://github.com/PirateEra/ai4mi_project
  contributions:
    Data Analysis and Problem Identification:
      - "Analyzed the voxel and slice distributions for all 40 participants, identifying severe class imbalance as a core limitation."
      - "Quantified that background voxels made up 98.95% of the data, with rare classes like Esophagus (0.05%) and Trachea (0.03%) being vastly underrepresented."
    Loss Function Experimentation:
      - "Identified that the baseline cross-entropy loss struggled with rare classes."
      - "Researched and implemented alternative loss functions to address the imbalance, including pure Dice loss and various hybrid (Dice + Cross Entropy) losses."
      - "Analyzed the trade-offs of these losses, noting that Dice loss improved performance on rare classes by focusing on overall shape rather than individual voxels."
    Model Improvement and Evaluation:
      - "Contributed to the development of the final 'Best Model,' which integrated multiple improvements (2.5D, Multi-Window, SE blocks, NAdam)."
      - "The final model achieved major gains in rare class segmentation."
  presentation_title: "Impact of Loss Functions on Dice & HD95"
  presentation_folder: "/assets/images/projects/medai/"

- id: pragmatics
  date: "06-2024"
  title: "Diverse Sampling for Referential Games"
  description: "Bachelor thesis on the impact of decoding algorithms on candidate diversity and accuracy in pragmatic reasoning."
  tools: "Python | NLP | LSTM | Decoding Algorithms"
  about: "This thesis investigates which decoding algorithm generates the best candidate utterances for a pragmatic speaker in a referential image game. The speaker must describe a target image to a listener to distinguish it from two distractors. The study compares Multinomial Sampling, Beam Decoding, and Diverse Beam Decoding on 'easy' (random) and 'hard' (visually/textually similar) distractors. The findings show that candidate diversity is crucial for success, and while DBD is the most promising for accuracy, it suffers from high computational costs compared to Multinomial Sampling."
  image: "/assets/images/sampling.png"
  url: "/pages/projects/pragmatics/"
  github-link: "https://github.com/PirateEra/ToM-Language-Acquisition"
  project_pdf_title: "Bachelor Thesis"
  project_pdf: "/assets/pdfs/sampling.pdf"
  contributions:
    Pragmatic Model Implementation:
      - "Implemented a full pragmatic reasoning pipeline based on the Rational Speech Act framework."
      - "Constructed a Pragmatic Speaker that samples candidates from a Literal Speaker (LSTM) and re-ranks them using a Literal Listener (ResNet/ROBERTa)."
    Decoding Algorithm Comparison:
      - "Implemented and systematically compared three distinct decoding algorithms: Multinomial Sampling, Beam Decoding, and Diverse Beam Decoding."
      - "Analyzed the trade-offs between candidate diversity (measured by Distinct n-grams) and model performance (measured by Accuracy and Entropy)."
    Experimental Design and Analysis:
      - "Designed an evaluation setup using the MS COCO dataset with two distinct difficulty levels: 'easy' (random distractors) and 'hard' (semantically similar distractors)."
      - "Created the 'hard' dataset by selecting distractors based on high visual and textual cosine similarity."
  media:
    - "/assets/images/sampling.png"

- id: shaders
  date: "10-2023"
  title: "WebGL Planet Shader"
  description: "An educational project implementing a 3D planet scene with custom vertex and fragment shaders, procedural noise, and texturing."
  tools: "JavaScript | WebGL | GLSL"
  about: "As part of the 'Graphics and Game Technology' course, this project involved using a JavaScript and WebGL framework to build a 3D scene from scratch. Starting with a blank framework, I implemented the procedural generation of a sphere mesh. I then wrote a series of GLSL vertex and fragment shaders to implement per-pixel Phong lighting, earth-map texturing, and procedural multi-layer noise for a dynamic cloud effect. Finally, I added a separate shader program to render an animated procedural starfield in the background."
  image: "/assets/images/shaders.png"
  url: "/pages/projects/shaders/"
  contributions:
    Geometry and Lighting:
      - "Procedurally generated a 3D sphere mesh, calculating all vertex positions, normals, and texture coordinates."
      - "Implemented Gouraud (per-vertex) shading and refactored it into a per-pixel Phong lighting model in the fragment shader."
    Procedural Texturing and Effects:
      - "Applied a 2D texture map of the Earth to the sphere using a sampler in the fragment shader."
      - "Wrote a GLSL noise function to generate multi-layered, procedural clouds that move over the planet's surface."
    Background Shader:
      - "Created a separate shader program to render a full-screen plane behind the planet."
      - "Used a noise function in the background shader to generate and animate a procedural starfield."
  media:
    - "/assets/images/shaders.png"
  code_snippets_title: Code Samples
  code_snippets:
    - title: "Noise function"
      content: |
        ```c
        float noise (in vec2 st) {
            vec2 i = floor(st);
            vec2 f = fract(st);
            // Four corners in 2D of a tile
            float a = random(i);
            float b = random(i + vec2(1.0, 0.0));
            float c = random(i + vec2(0.0, 1.0));
            float d = random(i + vec2(1.0, 1.0));
            // Smooth Interpolation
            vec2 u = smoothstep(0.,1.,f);
            // Mix 4 corner percentages
            return mix(a, b, u.x) + (c - a)* u.y * (1.0 - u.x) + (d - b) * u.x * u.y;
        }
        ```
    - title: "Star Fragment Shader"
      content: |
        ```c
        void main(void) {
            vec3 finalLightColor = vec3(0.0);
            float starSpeed = 0.0005;
            float currentCount = mod(uFrame * starSpeed, 60.0);

            float xPos = vPos[0] + currentCount;
        
            if (noise(vec2(xPos * 200.0, vPos[1] * 200.0)) > 0.93) {
                float noiseColor = noise(vec2(vPos[0] * 50.0, vPos[1] * 50.0));
                finalLightColor += vec3(noiseColor);
            }
        
            vec3 color = finalLightColor;
        
            gl_FragColor = vec4(color, 1.0);
        }
        ```

- id: raytrace
  date: "10-2023"
  title: "C Ray Tracer"
  description: "An educational project to build a ray tracer's core components, including multiple shading models, reflections, and a BVH acceleration structure."
  tools: "C | 3D Graphics | Ray Tracing"
  about: "This project, part of a 'Graphics and Game Technology' course, involved implementing the fundamental algorithms of a ray tracer from a provided C framework. I implemented the initial camera ray generation and various shading models, including Matte, Blinn-Phong, and recursive reflections, complete with shadow casting. A major component was performance optimization, where I implemented an efficient traversal algorithm for a Bounding Volume Hierarchy (BVH) to accelerate ray-triangle intersections for complex 3D models. Finally, I added anti-aliasing to improve final image quality."
  image: "/assets/images/final_result_raytrace.png"
  url: "/pages/projects/raytrace/"
  contributions:
    Core Ray Tracing and Optimization:
      - "Wrote the core logic for camera ray generation, shooting rays from the camera's position through the center of each pixel."
      - "Implemented a BVH traversal algorithm to drastically speed up intersection tests on complex models."
      - "Optimized BVH traversal by implementing branch pruning, skipping nodes that are farther than the closest intersection already found."
    Shader and Lighting Implementation:
      - "Implemented a Matte shader to compute surface color based on light contribution."
      - "Added shadow rays to the shaders, checking for occlusions between intersection points and light sources."
      - "Built a Blinn-Phong shader to render metallic-like surfaces by calculating ambient, diffuse, and specular lighting components."
      - "Implemented a recursive reflection shader that combined 75% matte shading with 25% reflected color, obtained by tracing new rays recursively."
    Image Quality:
      - "Added anti-aliasing (AA) by shooting and averaging four rays per pixel through sub-pixel centers to smooth jagged edges."
  presentation_title: "Rendered Images"
  presentation_folder: "/assets/images/projects/raytrace/"
  code_snippets_title: Code Samples
  code_snippets:
    - title: "Matte Shader"
      content: |
        ```c
        shade_matte(intersection_point ip)
        {
            float c = 0;
            float shadow_ray_origin_offset = 1.0e-3;
            vec3 normal = ip.n;
        
            for (int i = 0; i < scene_num_lights; i++) {
                light scene_light = scene_lights[i];
        
                float intensity = scene_light.intensity;
                vec3 direction = v3_normalize(v3_subtract(scene_light.position, ip.p));
        
                float point_light_contribution = 0;
        
                vec3 shadow_ray_origin = v3_add(ip.p, v3_multiply(direction, shadow_ray_origin_offset));
        
                if (!shadow_check(shadow_ray_origin, direction)) {
                    point_light_contribution = intensity * fmax(0, v3_dotprod(normal, direction));
                }
        
                c += point_light_contribution;
        
                if (c >= 1) {
                    c = 1;
                    break;
                }
            }
            c = fmin(1, c + scene_ambient_light);
            return v3_create(c, c, c);
        }
        ```
    - title: "Part of Phong Shader"
      content: |
        ```c
        light scene_light = scene_lights[i];

        float intensity = scene_light.intensity;
        vec3 direction = v3_normalize(v3_subtract(scene_light.position, ip.p));

        vec3 halfway_vec = v3_normalize(v3_add(ip.i, direction));

        float diffuse_contribution = 0;
        float specular_contribution = 0;

        vec3 shadow_ray_origin = v3_add(ip.p, v3_multiply(direction, shadow_ray_origin_offset));

        if (!shadow_check(shadow_ray_origin, direction)) {
            diffuse_contribution = intensity * fmax(0, v3_dotprod(normal, direction));
            specular_contribution = intensity * powf(v3_dotprod(normal, halfway_vec), alpha);
        }

        c_diffuse += diffuse_contribution;
        c_specular += specular_contribution;
        ```
